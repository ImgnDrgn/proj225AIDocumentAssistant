# proj 225 文档问答机器人

## 小组成员

|姓名|学号|贡献|
|:--:|:--:|:--:|
|陈进康|1120233181|33%|
|古俊逵|1120234020|33%|
|吴宣霖|1120234017|33%|

## 综述

小组一开始选题为`proj 285 安兔兔存储跑分测试`，但是由于技术原因 *（按文档刷机之后无法开机）* 以及测试机仅供比赛期间使用，最终在尝试一段时间后决定更换选题为`proj 225 文档问答机器人`。

题目要求根据[deepin系统wiki](https://wiki.deepin.org)上的中文教程，编写能根据其内容回答相关问题的机器人。

我们的预期流程是：

1. 首先寻找 qa 模型使用的流程，根据其内容尝试进行复现
2. 随后将 wiki 内容转换为数据集用于模型的微调
3. 构建知识库并与模型连接

项目在 github 上创建了[仓库](https://github.com/ImgnDrgn/proj225AIDocumentAssistant)，用于成员间同步工作进度以及相关文件，并记录了每次[会议的内容](https://github.com/ImgnDrgn/proj225AIDocumentAssistant/blob/8bf112e7c0235dccc44a41e4c9fd28ef0100ff1f/docs/%E8%BF%9B%E5%BA%A6%E8%AE%B0%E5%BD%95.md)。

## 具体过程

以下将分阶段对我们的工作进行说明。

### 复现

我们根据给出的参考链接，在 HuggingFace *(以下简称为 hf)* 上找到了[问答模型教程](https://huggingface.co/docs/transformers/main/en/tasks/question_answering)，然后成员分别根据教程的内容进行复现。复现过程中首先进行了相关环境的配置，然后编写了脚本[HF_QA.py](https://github.com/ImgnDrgn/proj225AIDocumentAssistant/blob/3a5ce10625e4e2fc72f667207cd3932c18245aa3/%E5%A4%8D%E7%8E%B0/HF_QA.py)来进行本地模型的微调。

脚本内容如下：
```python
# 导入必要的库
from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, AutoTokenizer, default_data_collator
from datasets import load_dataset
import numpy as np
import torch
import os

# 设置环境
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# 在代码开头添加选择逻辑
model_path = "distilbert/distilbert-base-uncased"  # 默认用原始模型
if os.path.exists("./qa_model/config.json"):  # 检查是否有已保存的模型
    answer = input("检测到已保存的模型，是否加载？(y/n): ")
    if answer.lower() == "y":
        model_path = "./qa_model"

# 1. 加载模型和tokenizer
model = AutoModelForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased").to(device)
tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
#使用之前模型
model_path = "./qa_model" if os.path.exists("./qa_model/config.json") else "distilbert/distilbert-base-uncased"
model = AutoModelForQuestionAnswering.from_pretrained(model_path).to(device)

tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")

# 2. 加载并预处理数据集（使用更小的子集加速训练）
squad = load_dataset("squad")

# 简化版预处理函数
def preprocess_function(examples):
    tokenized_inputs = tokenizer(
        examples["question"],
        examples["context"],
        truncation="only_second",
        max_length=256,  # 减少长度加速处理
        stride=64,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length"
    )
    
    # 处理答案位置
    sample_mapping = tokenized_inputs.pop("overflow_to_sample_mapping")
    offset_mapping = tokenized_inputs.pop("offset_mapping")
    
    tokenized_inputs["start_positions"] = []
    tokenized_inputs["end_positions"] = []
    
    for i, offsets in enumerate(offset_mapping):
        input_ids = tokenized_inputs["input_ids"][i]
        cls_index = input_ids.index(tokenizer.cls_token_id)
        sample_index = sample_mapping[i]
        answers = examples["answers"][sample_index]
        
        if len(answers["answer_start"]) == 0:
            tokenized_inputs["start_positions"].append(cls_index)
            tokenized_inputs["end_positions"].append(cls_index)
        else:
            start_char = answers["answer_start"][0]
            end_char = start_char + len(answers["text"][0])
            sequence_ids = tokenized_inputs.sequence_ids(i)
            
            # 找到答案的token范围
            token_start_index = 0
            while sequence_ids[token_start_index] != 1:
                token_start_index += 1
                
            token_end_index = len(input_ids) - 1
            while sequence_ids[token_end_index] != 1:
                token_end_index -= 1
                
            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
                tokenized_inputs["start_positions"].append(cls_index)
                tokenized_inputs["end_positions"].append(cls_index)
            else:
                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
                    token_start_index += 1
                tokenized_inputs["start_positions"].append(token_start_index - 1)
                
                while offsets[token_end_index][1] >= end_char:
                    token_end_index -= 1
                tokenized_inputs["end_positions"].append(token_end_index + 1)
    
    return tokenized_inputs

# 使用更小的数据集
train_dataset = squad["train"].select(range(2000))  # 进一步减少到500个样本
eval_dataset = squad["validation"].select(range(100))  # 100个验证样本

# 预处理数据集
tokenized_train = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)
tokenized_eval = eval_dataset.map(preprocess_function, batched=True, remove_columns=eval_dataset.column_names)

# 3. 设置训练参数
training_args = TrainingArguments(
    output_dir="./qa_model",
    eval_steps=100,  # 每100步评估一次
    per_device_train_batch_size=8 if device == "cuda" else 4,
    per_device_eval_batch_size=8 if device == "cuda" else 4,
    learning_rate=3e-5,
    weight_decay=0.01,
    num_train_epochs=2,  # 减少训练轮次
    save_total_limit=1,
    logging_steps=20,
    disable_tqdm=False  # 显示进度条
)

# 4. 初始化Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    data_collator=default_data_collator,
    tokenizer=tokenizer
)

# 5. 开始训练
print("开始训练...")
trainer.train()

# 6. 评估
eval_results = trainer.evaluate()
print("评估结果:", eval_results)

# 7. 保存模型（用于后续测试）
model.save_pretrained("./qa_model")
tokenizer.save_pretrained("./qa_model")

# 8. 测试单样本预测
print("\n===== 测试单样本预测 =====")
sample = eval_dataset[0]
inputs = tokenizer(
    sample["question"],
    sample["context"],
    return_tensors="pt",
    truncation=True,
    max_length=256
).to(device)  # 确保输入在正确设备上

with torch.no_grad():
    outputs = model(**inputs)

answer_start = torch.argmax(outputs.start_logits)
answer_end = torch.argmax(outputs.end_logits) + 1
answer_tokens = inputs["input_ids"][0][answer_start:answer_end]
answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)

print(f"问题: {sample['question']}")
print(f"上下文片段: {sample['context'][:100]}...")
print(f"\n真实答案: {sample['answers']['text'][0]}")
print(f"预测答案: {answer}")
print(f"答案位置: [{answer_start.item()}, {answer_end.item()}]")

```

微调生成的模型文件比较大，经过较长时间的研究之后，最终采用子模块的方式，将文件上传至仓库中[qa_model @ 1b487ea](https://github.com/SuperKUIKUI/qa_model/tree/1b487ea9a44c6f267d8a0975c3e4fce293b2edd7)。

### 生成数据集

#### 工具选择

由于 hf 的 qa 模型微调时仅支持`SQuAD`格式的数据集，所以我们需要首先对 wiki 的文档进行格式的转换。

考虑到文档的数目极其庞大，纯人工转换在时间上是不现实的，所以决定采用ai工具进行转换。

我们选取了4个国产的AI助手：deepseek、腾讯元宝deepseek、腾讯元宝hunyuan、豆包。我们分别选取了两篇文档[深度云打印](https://github.com/linuxdeepin/wiki.deepin.org/blob/0a2b9eabe192dac17e5da31e01b3f3ee6fe97d7f/01_%E8%BD%AF%E4%BB%B6wiki/00_GUI%E8%BD%AF%E4%BB%B6/01_deepin%E5%BC%80%E5%8F%91%E7%9A%84%E8%BD%AF%E4%BB%B6/%E6%B7%B1%E5%BA%A6%E4%BA%91%E6%89%93%E5%8D%B0.md#L2)*（短）*、[显卡](https://github.com/linuxdeepin/wiki.deepin.org/blob/0a2b9eabe192dac17e5da31e01b3f3ee6fe97d7f/02_%E7%A1%AC%E4%BB%B6wiki/00_%E7%A1%AC%E4%BB%B6%E7%9F%A5%E8%AF%86/05_%E6%98%BE%E5%8D%A1/%E6%98%BE%E5%8D%A1.md)*（长）*，交给4个ai，让其转换为`SQuAD`格式的数据集。前者的结果存在[测试1](https://github.com/ImgnDrgn/proj225AIDocumentAssistant/tree/3a5ce10625e4e2fc72f667207cd3932c18245aa3/%E6%95%B0%E6%8D%AE%E9%9B%86/%E5%B7%A5%E5%85%B7%E9%80%89%E6%8B%A9%E8%AF%84%E4%BC%B0/%E6%B5%8B%E8%AF%951)，后者结果存放在[测试2](https://github.com/ImgnDrgn/proj225AIDocumentAssistant/tree/3a5ce10625e4e2fc72f667207cd3932c18245aa3/%E6%95%B0%E6%8D%AE%E9%9B%86/%E5%B7%A5%E5%85%B7%E9%80%89%E6%8B%A9%E8%AF%84%E4%BC%B0/%E6%B5%8B%E8%AF%952)。

在测试1结束后，发现元宝hunyuan模型表现明显较差，所以不对其进行长文档的测试，直接淘汰。

在两轮测试后，成员对其生成内容进行了评估。评估的内容包括：格式、问题数、分段数、有效问题数。对于格式方面，由于我们的需求较为简单，所以只需要`SQuAD v1.1`的格式即可 *（无需无答案问题）*，另外每个问答对要有一个唯一、独特的`id`；对于问题总数，这在一定程度上反映了对于原文内容的覆盖程度；分段数（context数）则是考虑到，如果一个段落过程会导致训练效果不好；有效问题则由于生成内容中，可能会针对无关紧要的内容进行提问，或者对于同一点进行重复的提问，其判定由成员经过讨论之后确定。

两轮测试的具体内容详见[文档](https://github.com/ImgnDrgn/proj225AIDocumentAssistant/blob/70ddbedeeac026965a35fb562b6adda922ad44d3/%E6%95%B0%E6%8D%AE%E9%9B%86/%E5%B7%A5%E5%85%B7%E9%80%89%E6%8B%A9%E8%AF%84%E4%BC%B0/README.md)。考虑到单一ai可能存在较多的遗漏，最终决定采用豆包、元宝deepseek进行数据集生成，然后将其进行合并。生成时采用的提示词如下：
```
转换为SQuAD数据集1.0格式（不允许出现version、is_impossible等字段），要求严格按照格式，问答对要尽可能覆盖原文内容，且context不能过长，title见markdown开头title处
```

出于效率考虑，统一文档的合并还是采用ai进行，我们采用的是deepseek。尽管在分别提问时，都可能出现无效/无关问题，但是出于保证覆盖率优先的考虑，在合并时予以保留，并对格式、问答对id进行统一调整。采用的提示词如下：
```
这是关于同一篇文档的SQuAD格式的数据集，合并这两个数据集，要求：
1.只有一个title（即原文档的标题）
2.相同意思的问答对只保留一个
3.不同的问答对均保留
4.问答对id格式统一为"5-41-1"，"5-41-2"，……，其中"5-41-"固定不变
5.统一按照1.0格式，去除version、is_impossible等字段
6.不允许出现注释等不符合json语法规范的内容，涉及到的字符串中如果包含控制字符需要转义
```

#### 转换范围

由于 wiki 中部分内容偏向于社区构建，与Deepin系统使用偏离较大，所以在转换时进行跳过。最终转换的范围是：
*注：以下目录名来自[DeepinWiki官网](https://wiki.deepin.org/zh/home)*
- 01_软件wiki
- 02_硬件wiki
- 04_常见问题FAQ
- 05_HOW-TO ***其中子目录 06_参与deepin贡献相关，仅转换 deepin内测指南 ***
- 待分类

#### 编号及命名说明

对于以上大目录进行了重新编号，依次编号为`1-`、`2-`、…… 、`5-`，对于其中处于不同层级的文档统一按在官网目录中出现的顺序进行编号。对于大目录下文档较少的，文档编号如`-01-`、`-11-`；大目录下文档较多的，文档编号如`-001-`、`-112-`。

文档名格式为`大目录编号-文档编号-文档标题.json`。文档中的问答对id编号格式为`大目录编号-文档编号-问答对序号`，如上文中合并的提示词所示。

> 特殊情况:
> - 对于目录中有条目，但是wiki的仓库中未找到对应文档的，放上空json文件用于占位 *实际分工时未逐一进行核查，在进行工作时才发现存在这样的情况*
> - 仓库中文档名和官网中文档标题不对应的，按照官网标题（即md开头表格title处内容）处理
> - 部分文档在分工或者转换时有遗漏，编号可能会进行重新调整

#### 分工方式

一开始先每人分配较少的内容和较为宽松的时间来进行转换。后面考虑到剩余的文档数量庞大，如果按照这个速度，可能后续剩余时间不足以完成剩余阶段内容，因此多次调整速度。最终固定为每人5天200篇为一个小周期，每个小周期结束后会进行工作量的检查，确保进度。在整个转换过程中不断定期对内容进行调整和规范。

#### 数据集合并

编写了脚本[combine.py](https://github.com/ImgnDrgn/proj225AIDocumentAssistant/blob/006e1a8d713e7eeac104f0b74a902c83b7ff8290/%E6%95%B0%E6%8D%AE%E9%9B%86/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%90%88%E5%B9%B6/combine.py)，来合并生成的数据集为一个json文件，其内容如下
```python
import os, json, sys


def combine_all_json(doc_dir: str, combined_file: str):
    """将所有json文件合并为一个，从而用于模型训练

    Args:
        doc_dir (str): 待合并的文档所在的目录，注意：所有文档须放在同一目录下，该目录下不能有子目录等其他内容
        combined_file (str): 合并后的json文件路径，注意：以.json结尾
    """
    with open(
        combined_file, "w", encoding="utf-8"
    ) as combined:  # 打开合并后的文件，用于写入

        data = (
            []
        )  # 用于存放"data"键下所有内容，每一篇文档的内容都是一个字典，包含"title"和"paragraphs"两个键。这个字典作为data列表的元素
        title_dict = (
            dict()
        )  # 用于存放所有出现过的标题及其出现次数，从而处理某些文档中"title"重复的问题

        for filename in os.listdir(
            doc_dir
        ):  # 列出目录下所有文件（未检测是否是json、是否是有效目录，使用时需要将所有json文档放到一个有效目录下）

            file_path = os.path.join(doc_dir, filename)  # 组成文件完整路径

            # 只读打开要处理的json文件
            with open(file_path, "r", encoding="utf-8") as file:
                # 可能存在wiki文档缺失，用空文件占位的情况
                try:
                    # 对于非空文件
                    json_content = json.load(file)  # 将json文件内容转换成字典
                    try:
                        # 绝大部分应该是有data键的，但是部分可能一上来就直接"title"和"paragraphs"了
                        # 对于有"data"键的，其值是只有一个元素的列表，直接和data列表合并

                        # 合并前需要查看这个文档的"title"是否出现过了
                        title = json_content["data"][0][
                            "title"
                        ]  # 读取json中"title"的内容
                        if title in title_dict:
                            # 如果已经出现过，计数+1，并且修改"title"
                            title_dict[title] = title_dict[title] + 1
                            title = f"{title}-{title_dict[title]}"  # 名字改为"原title-出现次数"
                            json_content["data"][0][
                                "title"
                            ] = title  # 修改json_content中的"title"
                        else:
                            # 对于首次出现的标题，将其加入title_dict，并计数为1
                            title_dict[title] = 1

                        # 再合并本篇文档
                        data = data + json_content["data"]
                    except KeyError:
                        # 对于没有"data"键的，上来是"title和"paragraphs"，直接将json_content作为元素添加到data列表中

                        # 同样合并前先判断是否出现过
                        title = json_content["title"]
                        if title in title_dict:
                            title_dict[title] = title_dict[title] + 1
                            title = f"{title}-{title_dict[title]}"
                            json_content["title"] = title
                        else:
                            title_dict[title] = 1

                        data.append(json_content)

                except json.JSONDecodeError:
                    pass  # 对于空文件，不进行操作，自行关闭后处理下一个即可

        # 所有文件提取完毕后，将其分装成json格式的字典
        json.dump(dict(data=data), combined)


if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("参数数量错误！")
    else:
        combine_all_json(sys.argv[1], sys.argv[2])
        print("合并成功！")

```
其使用方法为
```
python combine.py [doc_dir] [combined_file]
```
其中`dic_dir`为所有单个文档的数据集的目录（无目录嵌套和其他内容），`combined_file`为输出的文件名。

尽管在合并两个ai生成内容时对格式进行了一定的统一，但是还是存在个别格式不统一的情况：
对于大部分文档，整个json有一个键"data"，其值是列表，列表中元素的键是"title"和"paragraphs"；然而部分文档没有"data"，"title"和"paragraphs"直接作为json的两个键。
这个问题在脚本中进行了考虑并修复了。

另外，在后续使用时，发现部分文档对应的部分内部存在问题，会有"context"整个缺失的情况。对于这些部分，后续直接重新生成数据集并手动替换内容，现在仓库中的数据集均为已修复版本。

### 模型选择

由于我们实际的需求是采用本地中文数据集，对于支持中文qa的模型进行微调，和上文中教程的内容还存在出入，因此不能完全按照其内容进行，需要重新确定具体操作步骤。

我们根据实际的需求，重新编写了[微调流程文档](https://github.com/ImgnDrgn/proj225AIDocumentAssistant/blob/006e1a8d713e7eeac104f0b74a902c83b7ff8290/%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9E%8B/%E5%8F%82%E8%80%83%E8%BF%87%E7%A8%8B/%E5%BE%AE%E8%B0%83%E6%B5%81%E7%A8%8B.md)。微调常用的脚本[finetuneTest.py](https://github.com/ImgnDrgn/proj225AIDocumentAssistant/blob/006e1a8d713e7eeac104f0b74a902c83b7ff8290/%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9E%8B/%E5%8F%82%E8%80%83%E8%BF%87%E7%A8%8B/finetuneTest.py)如下：
```python

import sys
from datasets import load_dataset, Dataset
import itertools
from transformers import AutoTokenizer
from transformers import DefaultDataCollator
from transformers import AutoModelForQuestionAnswering, Trainer, TrainingArguments
import evaluate
import numpy as np

# import torch
import os

# 禁用符号链接警告
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# 解析命令行参数
if len(sys.argv) != 3:
    print("正确用法：python finetune.py <模型名称> <模型保存路径>")
    print("示例：python finetune.py FlagAlpha/Llama2-Chinese-13b-Chat ./FlagAlpha13b")
    sys.exit(1)

MODEL_NAME = sys.argv[1]  # 传入的预训练模型名
OUTPUT_DIR = sys.argv[2]  # 传入的模型保存路径


# ----------------------------------- 加载数据集 ---------------------------------- #
# 加载原始数据
raw_data = load_dataset("json", data_files="DeepinWiki.json", split="train")


# 展平+格式转换函数
def process_data(example):
    flat_samples = []
    # 提取title
    title = example["data"].get("title", "Unknown")
    for para in example["data"]["paragraphs"]:
        context = para["context"]
        for qa in para["qas"]:
            # 直接重组answers格式（无需筛选，数据均有效）
            target_answers = {
                "answer_start": [qa["answers"][0]["answer_start"]],  # 整型→单元素列表
                "text": [qa["answers"][0]["text"]],  # 字符串→单元素列表
            }
            flat_samples.append(
                {
                    "context": context,
                    "question": qa["question"],
                    "answers": target_answers,  # 列表->字典
                    "id": qa["id"],
                    "title": title,
                }
            )
    return {"samples": flat_samples}


# 数据处理+展平
mapped_data = raw_data.map(process_data)
flattened_samples = list(itertools.chain(*mapped_data["samples"]))

# 重建Dataset
flattened_data = Dataset.from_dict(
    {
        "context": [s["context"] for s in flattened_samples],
        "question": [s["question"] for s in flattened_samples],
        "answers": [s["answers"] for s in flattened_samples],
        "id": [s["id"] for s in flattened_samples],
        "title": [s["title"] for s in flattened_samples],
    }
)

# 拆分为训练集和测试集，测试集占0.2
deepin_dataset = flattened_data.train_test_split(test_size=0.2)

# print(squad["train"][0])  # 验证结果


# ------------------------------------ 预处理 ----------------------------------- #
# 使用模型配套的分词器
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)


# 预处理函数
def preprocess_function(examples):
    questions = [q.strip() if isinstance(q, str) else "" for q in examples["question"]]
    contexts = [c.strip() if isinstance(c, str) else "" for c in examples["context"]]

    inputs = tokenizer(
        questions,
        contexts,
        max_length=512,
        truncation="only_second",
        stride=128,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
        add_special_tokens=True,
    )

    offset_mapping = inputs.pop("offset_mapping")
    sample_map = inputs["overflow_to_sample_mapping"]
    answers = examples["answers"]
    start_positions = []
    end_positions = []

    for i in range(len(offset_mapping)):
        sample_idx = sample_map[i]
        current_answer = answers[sample_idx]

        start_char = current_answer["answer_start"][0]
        end_char = start_char + len(current_answer["text"][0])

        sequence_ids = inputs.sequence_ids(i)
        if sequence_ids is None:
            start_positions.append(-100)
            end_positions.append(-100)
            continue

        context_start = next(
            (idx for idx, seq_id in enumerate(sequence_ids) if seq_id == 1), None
        )
        # 反向找最后一个属于context的token（seq_id=1）
        context_end = next(
            (
                idx
                for idx, seq_id in reversed(list(enumerate(sequence_ids)))
                if seq_id == 1
            ),
            None,
        )

        if context_start is None or context_end is None:
            start_positions.append(-100)
            end_positions.append(-100)
            continue

        if (
            offset_mapping[i][context_end][1] < start_char
            or offset_mapping[i][context_start][0] > end_char
        ):
            start_positions.append(-100)
            end_positions.append(-100)
        else:
            start_token = context_start
            while (
                start_token <= context_end
                and offset_mapping[i][start_token][0] <= start_char
            ):
                start_token += 1
            start_positions.append(start_token - 1)

            end_token = context_end
            while (
                end_token >= context_start
                and offset_mapping[i][end_token][1] >= end_char
            ):
                end_token -= 1
            end_positions.append(end_token + 1)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions

    return inputs


# 应用预处理（批量处理+移除文本列）
tokenized_deepin = deepin_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=deepin_dataset["train"].column_names,
)

# 生成训练批次数据
data_collator = DefaultDataCollator()

# ------------------------------------ 训练 ------------------------------------ #
# 加载模型
model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME, device_map="auto")
# 加载模型后打印状态（确认是否成功）
print("模型加载成功！")
print(f"模型设备：{next(model.parameters()).device}")  # 应输出 cuda:0（GPU）

# 配置TrainingArguments
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,  # 模型保存路径
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=False,  # 不需要推送到hf的hub
    save_strategy="epoch",
    load_best_model_at_end=True,  # 训练结束后加载最优模型
    metric_for_best_model="loss",  # 以损失值作为最优判断标准
    greater_is_better=False,  # 损失值越小越好
    save_total_limit=3,  # 限制保留的最多3个快照
    logging_steps=10,  # 每10步记录一次日志
    logging_dir="./logs",  # 日志目录
    report_to="none",  # 禁用所有集成报告器
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_deepin["train"],
    eval_dataset=tokenized_deepin["test"],
    processing_class=tokenizer,
    data_collator=data_collator,
)

# 启动训练
trainer.train()

# 保存训练好的模型
best_model_path = f"{OUTPUT_DIR}/best_model"
trainer.save_model(best_model_path)
tokenizer.save_pretrained(best_model_path)
print(f"最优模型已保存到：{best_model_path}")

```

在文档中采用`liam168/qa-roberta-base-chinese-extractive`模型，并在微调结束后收集如下信息，用于模型比较。
```
{'train_runtime': 3188.1479, 'train_samples_per_second': 11.173, 'train_steps_per_second': 2.794, 'train_loss': 2.8489648055130883, 'epoch': 3.0}
```

后续再根据hf上的点赞量、下载量选择了`uer/roberta-base-chinese-extractive-qa`和`timl0l/mdeberta-v3-base-squad2`进行微调，并收集了训练数据。

三个模型的结果数据如下：
|model name|train_runtime|train_samples_per_second|train_steps_per_second|**train_loss**|epoch|
|:---:     |:---:        |:---:                   |:---:                 |:---:         |:---:|
|`lim68/qa-roberta-base-chiese-extractive`|3188.1479|11.173|2.794|**2.8489648055130883**|3.0  |
|`timpal0l/mdeberta-v3-base-squad2`|4511.445|7.795|1.949|**2.6428933777554806**|3.0           |
|`uer/roberta-base-cinese-extractive-qa`|1629.4328|21.865|5.466|**2.8305770402486257**|3.0    |

这里主要通过`train_loss`来对性能进行简单的判断，最终决定选用`timpal0l/mdeberta-v3-base-squad2`模型。
*这里考虑到后面知识库的使用，暂时没有将该模型上传到github，后面将其上传到了hf*

### 知识库构建和使用

#### 原文文档准备

在构建知识库的时候一般是正常的文本或者表格文档，而非`SQuAD`格式的json文件，所以还需要准备一个原文文档。

编写了脚本[contextExtract.py](https://github.com/ImgnDrgn/proj225AIDocumentAssistant/blob/006e1a8d713e7eeac104f0b74a902c83b7ff8290/%E7%9F%A5%E8%AF%86%E5%BA%93%E6%9E%84%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8/contextExtract.py)，用于从合并并修复了的数据集`DeepinWiki.json`中提取原文，组成原文文档合集。其内容如下
```python
import json

filepath = "../数据集/数据集合并/DeepinWiki.json"
endfile = "./DeepinWiki.txt"

with open(endfile, "w", encoding="utf-8") as fe:
    with open(filepath, "r", encoding="utf-8") as fw:
        wiki = json.load(fw)
        data_list = wiki["data"]
        for data in data_list:
            fe.write("标题：" + data["title"] + "\n")
            paragraphs_list = data["paragraphs"]
            for paragraphs in paragraphs_list:
                context = paragraphs["context"]
                fe.write(context + "\n\n")

```

生成的原文文档合集为[DeepinWiki.txt](https://github.com/ImgnDrgn/proj225AIDocumentAssistant/blob/006e1a8d713e7eeac104f0b74a902c83b7ff8290/%E7%9F%A5%E8%AF%86%E5%BA%93%E6%9E%84%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8/DeepinWiki.txt)。

#### 知识库构建尝试

起初我们尝试用**ollama**和**AnythingLLM**来构建知识库，前者用于跑模型，后者用于创建知识库并连接到模型。经过尝试之后失败了。

我们发现AnythingLLM可以直接使用hf上的模型，所以我们将选取的[微调后的模型](https://huggingface.co/dddddd10/timl0l/tree/main)上传到了hf，然后在hf上租用服务器创建推理端点，用于连接到AnythingLLM。创建推理端点时，提示需要`handler.py`等文件，于是尝试编写，以下是最终成功的版本

requirements.txt
```
torch>=1.9.0
transformers>=4.20.0
sentencepiece
protobuf
```

handler.py
```python
import json
import torch
import logging
from transformers import AutoModelForQuestionAnswering, AutoTokenizer

# 设置日志
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

class EndpointHandler:
    """
    处理中文问答模型推理的自定义处理器
    专为 timpal0l/mdeberta-v3-base-squad2 模型与 AnythingLLM 集成设计
    符合 Hugging Face Inference Endpoints 的 Custom Handler 规范
    """
    
    def __init__(self, path=""):
        """
        初始化方法，path 参数是 Hugging Face 框架传递的模型路径
        """
        self.initialized = False
        self.model = None
        self.tokenizer = None
        self.device = None
        
        # 尝试初始化
        try:
            self.initialize()
        except Exception as e:
            logger.error(f"初始化失败: {str(e)}")
            raise

    def initialize(self):
        """
        初始化模型和分词器
        """
        # 设置设备
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"使用设备: {self.device}")
        
        # 加载模型和分词器 - 使用你的模型ID
        model_name = "timpal0l/mdeberta-v3-base-squad2"
        logger.info(f"正在加载模型: {model_name}")
        
        try:
            # 加载分词器
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # 加载模型
            self.model = AutoModelForQuestionAnswering.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32
            )
            
            # 将模型移动到设备
            self.model = self.model.to(self.device)
            self.model.eval()  # 设置为评估模式
            
            self.initialized = True
            logger.info("模型和分词器加载成功")
            
        except Exception as e:
            logger.error(f"模型加载失败: {str(e)}")
            raise

    def __call__(self, data, **kwargs):
        """
        调用入口，Hugging Face 端点会调用这个方法
        """
        try:
            # 处理输入数据
            processed_data = self.preprocess(data)
            
            # 执行推理
            inference_result = self.inference(processed_data)
            
            # 处理后处理
            result = self.postprocess(inference_result)
            
            return result
            
        except Exception as e:
            logger.error(f"处理请求时出错: {str(e)}")
            return {"error": f"处理请求时出错: {str(e)}"}

    def preprocess(self, data):
        """
        预处理输入数据
        """
        try:
            # 解析请求数据
            if isinstance(data, (bytes, bytearray)):
                data = data.decode('utf-8')
            
            if isinstance(data, str):
                request_data = json.loads(data)
            else:
                request_data = data
            
            # 支持多种可能的输入格式
            if "inputs" in request_data:
                # Hugging Face 标准格式
                inputs = request_data["inputs"]
                question = inputs.get("question", "")
                context = inputs.get("context", "")
            elif "question" in request_data and "context" in request_data:
                # 直接包含 question 和 context 的格式
                question = request_data["question"]
                context = request_data["context"]
            elif "input" in request_data:
                # 通用输入格式
                input_data = request_data["input"]
                if isinstance(input_data, dict):
                    question = input_data.get("question", "")
                    context = input_data.get("context", "")
                else:
                    # 尝试解析字符串格式的输入
                    question = input_data
                    context = ""
            else:
                # 默认提取方式
                question = request_data.get("question", request_data.get("query", ""))
                context = request_data.get("context", request_data.get("text", ""))
            
            logger.info(f"收到问题: {question[:100]}...")
            logger.info(f"上下文长度: {len(context)} 字符")
            
            # 对输入进行分词
            inputs = self.tokenizer(
                question,
                context,
                add_special_tokens=True,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding="max_length",
                return_overflowing_tokens=False
            )
            
            # 移动到设备
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            return {
                "model_inputs": inputs,
                "question": question,
                "context": context
            }
            
        except Exception as e:
            logger.error(f"预处理错误: {str(e)}")
            raise

    def inference(self, inputs):
        """
        执行模型推理
        """
        try:
            model_inputs = inputs["model_inputs"]
            
            with torch.no_grad():
                # 模型推理
                outputs = self.model(**model_inputs)
                
                # 获取开始和结束位置的概率
                start_logits = outputs.start_logits
                end_logits = outputs.end_logits
                
                # 转换为概率
                start_probs = torch.softmax(start_logits, dim=1)
                end_probs = torch.softmax(end_logits, dim=1)
                
                # 获取最可能的开始和结束位置
                start_idx = torch.argmax(start_probs, dim=1)
                end_idx = torch.argmax(end_probs, dim=1)
                
                # 计算置信度分数
                confidence = (start_probs[0, start_idx] * end_probs[0, end_idx]).item()
                
                # 提取答案
                answer_tokens = model_inputs["input_ids"][0][start_idx:end_idx+1]
                answer = self.tokenizer.decode(answer_tokens, skip_special_tokens=True)
                
                # 处理无答案的情况 (SQuAD 2.0 特性)
                if answer.strip() == "" or confidence < 0.01:
                    answer = ""
                    confidence = 0.0
                    start_idx = 0
                    end_idx = 0
                
                logger.info(f"模型预测答案: '{answer}' (置信度: {confidence:.4f})")
                
                return {
                    "answer": answer,
                    "score": float(confidence),  # 转换为 Python float 类型
                    "start": start_idx.item(),
                    "end": end_idx.item(),
                    "model": "timpal0l/mdeberta-v3-base-squad2"
                }
                
        except Exception as e:
            logger.error(f"推理错误: {str(e)}")
            return {
                "answer": "",
                "score": 0.0,
                "start": 0,
                "end": 0,
                "error": str(e)
            }

    def postprocess(self, inference_output):
        """
        后处理模型输出，确保符合 AnythingLLM 的预期格式
        """
        # 确保输出格式与 AnythingLLM 兼容
        result = {
            "answer": inference_output.get("answer", ""),
            "score": inference_output.get("score", 0.0),
            "start": inference_output.get("start", 0),
            "end": inference_output.get("end", 0)
        }
        
        # 添加错误信息（如果有）
        if "error" in inference_output:
            result["error"] = inference_output["error"]
            
        logger.info(f"最终输出: {json.dumps(result, ensure_ascii=False)}")
        return result
```


但是AnythingLLM这边总是显示
```
Could not respond to message.
> 404 Not Found
```

无奈我们只能考虑换用其他方法。最终采用faiss构建向量数据库作为知识库并使用。采用的是本地模型。

思路如下：

![流程](images\image.png)

最初的版本是直接在向量化的知识库中搜索上下文，但是效果并不好

![初版测试](images\0aa32c736bfa6ab38692400d00b27744.png)

修改为两级检索，效果稍微好一点

![两级1](images\cb7519fd841dd0bdb7f12f5d76d3b784.png)

![两级2](images\3fa57698c6a8f4c46a020519cea01bd6.png)

最终采用三级检索，但是效果并没有比较明显的提升。

![三级1](images\414cddebc0e3631c5bca2d6da32f9f2b.png)

![三级2](images\f472423be891008023e78a2e809bd154.png)

![三级3](images\fedfd22df24eedbdae0bd9546c957ce2.png)

最终版本的脚本[train.py](https://github.com/ImgnDrgn/proj225AIDocumentAssistant/blob/7f5d2f2061d586166f2f5d91820f230e7cbc6447/%E7%9F%A5%E8%AF%86%E5%BA%93%E6%9E%84%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8/train.py)如下
```python
import re
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline

# 1. 读取文档并切割（保持原来的按标题切割）
with open("DeepinWiki.txt", "r", encoding="utf-8") as f:
    text = f.read()

segments = re.split(r"标题：", text)
segments = [seg.strip() for seg in segments if seg.strip()]

docs = []
for seg in segments:
    lines = seg.splitlines()
    title = lines[0]
    content = "\n".join(lines[1:])
    docs.append({"title": title, "content": content})

# 2. 加载 sentence-transformers 模型
embedder = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# 3. 构建所有层级的索引
# 3.1 文档级索引（标题+内容）
doc_texts = [d["title"] + " " + d["content"] for d in docs]
doc_embeddings = embedder.encode(doc_texts, convert_to_numpy=True)
doc_index = faiss.IndexFlatL2(doc_embeddings.shape[1])
doc_index.add(doc_embeddings)

# 3.2 预构建段落级索引（为每个文档建立段落索引）
paragraph_indices = {}
paragraph_data = {}

for doc_idx, doc in enumerate(docs):
    # 按段落分割（多种分隔方式）
    paragraphs = re.split(r'\n\n+|\n•\s*|\n\d+[\.\)]\s*', doc["content"])
    paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p.strip()) > 20]
    
    if not paragraphs:
        paragraphs = [doc["content"]]
    
    para_embeddings = embedder.encode(paragraphs, convert_to_numpy=True)
    para_index = faiss.IndexFlatL2(para_embeddings.shape[1])
    para_index.add(para_embeddings)
    
    paragraph_indices[doc_idx] = para_index
    paragraph_data[doc_idx] = paragraphs

# 4. 加载 QA 模型
model_path = "./tim_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForQuestionAnswering.from_pretrained(model_path)
qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer, device=0)

def find_best_sentences(question, paragraph, top_k=3):
    """句子级检索：从段落中找出最相关的几个句子"""
    # 分句
    sentences = re.split(r'[.!?。！？]+', paragraph)
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
    
    if len(sentences) <= 1:
        return paragraph
    
    # 句子向量化
    sent_embeddings = embedder.encode(sentences, convert_to_numpy=True)
    sent_index = faiss.IndexFlatL2(sent_embeddings.shape[1])
    sent_index.add(sent_embeddings)
    
    # 检索最相关句子
    q_embedding = embedder.encode([question], convert_to_numpy=True)
    D, I = sent_index.search(q_embedding, min(top_k, len(sentences)))
    
    # 按相关性排序并组合
    best_sentences = [sentences[i] for i in I[0]]
    return " ".join(best_sentences)

def multi_level_retrieval(question, top_k_docs=3, top_k_paras=2, top_k_sents=3):
    q_embedding = embedder.encode([question], convert_to_numpy=True)
    D, I = doc_index.search(q_embedding, min(top_k_docs, len(docs)))
    
    best_contexts = []
    for doc_idx in I[0]:
        doc = docs[doc_idx]
        para_index = paragraph_indices[doc_idx]
        paragraphs = paragraph_data[doc_idx]

        D_para, I_para = para_index.search(q_embedding, min(top_k_paras, len(paragraphs)))

        for rank, para_idx in enumerate(I_para[0]):  # 用 rank 对齐
            paragraph = paragraphs[para_idx]
            best_sentences = find_best_sentences(question, paragraph, top_k_sents)

            best_contexts.append({
                'title': doc['title'],
                'content': best_sentences,
                'score': float(D_para[0][rank])  # 用 rank，而不是 para_idx
            })

    best_contexts.sort(key=lambda x: x['score'])
    return best_contexts


def ask(question):
    """改进的QA函数，使用多级检索"""
    # 多级检索获取最佳上下文
    contexts = multi_level_retrieval(question)
    
    if not contexts:
        return "抱歉，没有找到相关信息。"
    
    # 尝试在前几个最佳上下文中寻找答案
    best_answer = None
    best_score = 0
    
    for context in contexts[:3]:  # 尝试前3个最佳上下文
        try:
            result = qa_pipeline(question=question, context=context['content'])
            
            # 选择置信度最高的答案
            if result['score'] > best_score:
                best_score = result['score']
                best_answer = {
                    'answer': result['answer'],
                    'score': result['score'],
                    'title': context['title'],
                    'context': context['content']
                }
        except:
            continue
    
    if best_answer and best_score > 0.1:  # 设置置信度阈值
        return f"【{best_answer['title']}】 → {best_answer['answer']} (得分: {best_answer['score']:.3f})"
    else:
        # 如果QA模型没找到答案，返回最相关的上下文
        return f"【{contexts[0]['title']}】 → 未找到确切答案，相关上下文：{contexts[0]['content'][:200]}..."

# 交互部分保持不变
if __name__ == "__main__":
    print("改进版多级检索问答系统已启动，输入 exit/quit/q 退出")
    print("-" * 50)
    
    while True:
        q = input("你: ").strip()
        if q.lower() in ["exit", "quit", "q"]:
            break
        if not q:
            continue
            
        try:
            answer = ask(q)
            print("机器人:", answer)
        except Exception as e:
            print(f"机器人: 出错了 - {str(e)}")
        
        print()  # 空行
```